{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Pre Processing Notebook\n",
    "This Notebook deals with processing raw datasets from `product_datasets` folder and saves the cleaned datasets into `pre_processed_datasets` folder.\n",
    "The preprocessing steps include:\n",
    " - Converting the 'Day Index' column to a proper datetime format.\n",
    " - Removing duplicate rows to clean the data.\n",
    " - Filling any missing values using forward fill.\n",
    " \n",
    "After preprocessing, the cleaned datasets will be ready for merging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # For working with datasets\n",
    "import os # For working with paths/directories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1 : Defining Input and Output folders\n",
    "I have used relative paths for specifying the locations of the Input and Output folders. This ensures that the notebook works on any machine as long as the dataset files are placed in the expected directory structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Path Exists: True\n",
      "Output Path Exists: True\n"
     ]
    }
   ],
   "source": [
    "input_folder = '../datasets/product_datasets'\n",
    "output_folder = '../datasets/pre_processed_datasets'\n",
    "\n",
    "# Verifying existence of each folder.\n",
    "print(f\"Input Path Exists: {os.path.exists(input_folder)}\")\n",
    "print(f\"Output Path Exists: {os.path.exists(output_folder)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2 : List of dataset files\n",
    "The following datasets shall be processed:\n",
    "1. ProductA_google_clicks.xlsx\n",
    "2. ProductA_fb_impressions.xlsx\n",
    "3. ProductA.xlsx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = ['ProductA_google_clicks.xlsx','ProductA_fb_impressions.xlsx','ProductA.xlsx']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3 : Pre Process each dataset\n",
    "The foloowing procedures are carried out upon each dataset:\n",
    " - Load the data.\n",
    " - Convert `Day Index` into datetime format.\n",
    " - Remove duplicate rows.\n",
    " - Fill missing values using forward fill.\n",
    " - Save the cleaned datasets to \"pre_processed_datasets\" folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ProductA_google_clicks.xlsx...\n",
      "Preprocessed dataset saved as pre_processed_ProductA_google_clicks.xlsx\n",
      "\n",
      "Processing ProductA_fb_impressions.xlsx...\n",
      "Preprocessed dataset saved as pre_processed_ProductA_fb_impressions.xlsx\n",
      "\n",
      "Processing ProductA.xlsx...\n",
      "Preprocessed dataset saved as pre_processed_ProductA.xlsx\n",
      "\n",
      "All Datasets have been preprocessed successfully !!\n"
     ]
    }
   ],
   "source": [
    "for dataset in datasets:\n",
    "    print(f\"Processing {dataset}...\")  # Show which dataset is being processed\n",
    "\n",
    "    # Load the dataset\n",
    "    input_path = os.path.join(input_folder, dataset)\n",
    "    df = pd.read_excel(input_path)\n",
    "\n",
    "    # Convert 'Day Index' to datetime format\n",
    "    if 'Day Index' in df.columns:\n",
    "        df['Day Index'] = pd.to_datetime(df['Day Index'])\n",
    "    else:\n",
    "        print(f\"Warning: 'Day Index' column is missing in {dataset}. Skipping this file.\")\n",
    "        continue\n",
    "\n",
    "    # Remove duplicate rows\n",
    "    df = df.drop_duplicates()\n",
    "\n",
    "    # Fill missing values\n",
    "    df = df.ffill()  # Use forward fill to handle missing values\n",
    "\n",
    "    # Save the preprocessed dataset with the prefix \"pre_processed_\"\n",
    "    output_filename = f\"pre_processed_{dataset}\"\n",
    "    output_path = os.path.join(output_folder, output_filename)\n",
    "    df.to_excel(output_path, index=False)\n",
    "    print(f\"Preprocessed dataset saved as {output_filename}\\n\")\n",
    "\n",
    "\n",
    "print(\"All Datasets have been preprocessed successfully !!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
